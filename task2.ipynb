{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-family: 'Lexend', sans-serif; font-size: 36px;\">\n",
    "    <strong>Task 2 for KDAG 2025 Selections by Neel Gupta 24BT10040 </strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-family: 'Lexend', sans-serif; font-size: 30px;\">\n",
    "    <strong>Music Genre Clustering Analysis</strong>\n",
    "</div>\n",
    "<div style=\"text-align: center; font-family: 'Lexend', sans-serif; font-size: 24px;\">\n",
    "    <strong>Interactive Exploration of Song Keywords</strong>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Lexend', sans-serif; font-size: 24px;\">\n",
    "    <strong>1. Environment Setup</strong>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Lexend', sans-serif; font-size: 20px;\">\n",
    "    <strong>Install Requirements if not already present :</strong>\n",
    "</div>\n",
    "<div style=\"font-family: 'Lexend', sans-serif; font-size: 13px;\">\n",
    "    <strong>Breakdown of Dependencies and Installation guide:</strong><br>\n",
    "    pandas → For data manipulation and analysis. → <code>pip install pandas</code> <button onclick=\"navigator.clipboard.writeText('pip install pandas')\">Copy</button> <br>\n",
    "    numpy → For numerical operations and arrays. → <code>pip install numpy</code> <button onclick=\"navigator.clipboard.writeText('pip install numpy')\">Copy</button> <br>\n",
    "    matplotlib → For plotting and data visualization. → <code>pip install matplotlib</code> <button onclick=\"navigator.clipboard.writeText('pip install matplotlib')\">Copy</button> <br>\n",
    "    seaborn → For statistical data visualization. → <code>pip install seaborn</code> <button onclick=\"navigator.clipboard.writeText('pip install seaborn')\">Copy</button> <br>\n",
    "    collections (built-in) → No need to install separately. <br>\n",
    "    tqdm → For progress bars in loops. → <code>pip install tqdm</code> <button onclick=\"navigator.clipboard.writeText('pip install tqdm')\">Copy</button> <br>\n",
    "    ipywidgets → For interactive widgets in Jupyter. → <code>pip install ipywidgets</code> <button onclick=\"navigator.clipboard.writeText('pip install ipywidgets')\">Copy</button> <br>\n",
    "    <br>\n",
    "    <strong></strong><br>\n",
    "</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "from ipywidgets import interact, widgets, interact_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Lexend', sans-serif; font-size: 24px;\">\n",
    "    <strong>2. Data Preparation</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef931ae65b245d1a1f1ea03c9e9275e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=25, description='rows', max=50, min=5, step=5), Output()), _dom_classes=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact\n",
    "import pandas as pd\n",
    "\n",
    "# A function to load and preprocess the dataset\n",
    "# Combine the keyword columns into a single column called 'document' for text analysis \n",
    "def load_data(file_path):\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['document'] = df[['keyword_1', 'keyword_2', 'keyword_3']].apply(\n",
    "        lambda x: ' '.join(x.dropna()), axis=1\n",
    "    )\n",
    "    return df\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"TASK2_dataset.csv\"  # Update with your path\n",
    "df = load_data(file_path)\n",
    "\n",
    "# Display interactive data explorer to give a slider to select the number of rows to display\n",
    "@interact\n",
    "def show_data(rows=(5, 50, 5)):\n",
    "    return df.head(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Lexend', sans-serif; font-size: 24px;\">\n",
    "    <strong>3a. BOW Vectorizer Implementation</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not used in implementation\n",
    "import numpy as np\n",
    "class BOWVectorizer:\n",
    "    def __init__(self):\n",
    "        self.vocab = [] # Stores unique words from all documents\n",
    "    \n",
    "    def fit(self, documents):\n",
    "        # Split documents into individual words\n",
    "        tokenized = [doc.split() for doc in documents]\n",
    "\n",
    "        # Create vocabulary using set comprehension to get unique words\n",
    "        self.vocab = list({word for doc in tokenized for word in doc})\n",
    "        \n",
    "    def transform(self, documents):\n",
    "        # Create count vectors using nested list comprehensions\n",
    "        return np.array([\n",
    "            [doc.split().count(word) for word in self.vocab] # Word counts\n",
    "            for doc in documents\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Lexend', sans-serif; font-size: 24px;\">\n",
    "    <strong>3b. TF-IDF Implementation</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b99dbccf00d645e3abbbdef64d84a074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing documents:   0%|          | 0/147 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac47c9bb4a9d42de91578736e6622b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating DF:   0%|          | 0/147 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12a45abac5c34993a044363af114234a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming documents:   0%|          | 0/147 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class TFIDFVectorizer:\n",
    "    def __init__(self):\n",
    "        self.vocab = []\n",
    "        self.idf = {}\n",
    "        \n",
    "    def fit(self, documents):\n",
    "        df_counter = defaultdict(int)\n",
    "        tokenized_docs = [doc.split() for doc in tqdm(documents, desc=\"Processing documents\")]\n",
    "        unique_words = set(word for doc in tokenized_docs for word in doc)\n",
    "        self.vocab = list(unique_words)\n",
    "        \n",
    "        for doc in tqdm(tokenized_docs, desc=\"Calculating DF\"):\n",
    "            for word in set(doc):\n",
    "                df_counter[word] += 1\n",
    "                \n",
    "        total_docs = len(documents)\n",
    "        self.idf = {word: np.log((total_docs + 1) / (df_counter[word] + 1)) + 1 \n",
    "                   for word in self.vocab}\n",
    "    \n",
    "    def transform(self, documents):\n",
    "        vectors = []\n",
    "        for doc in tqdm(documents, desc=\"Transforming documents\"):\n",
    "            words = doc.split()\n",
    "            word_counts = {word: words.count(word) for word in self.vocab}\n",
    "            tf = {word: count/len(words) for word, count in word_counts.items()}\n",
    "            vector = [tf[word] * self.idf[word] for word in self.vocab]\n",
    "            vectors.append(vector)\n",
    "        return np.array(vectors)\n",
    "\n",
    "# Initialize and fit TF-IDF\n",
    "tfidf = TFIDFVectorizer()\n",
    "tfidf.fit(df['document'])\n",
    "tfidf_vectors = tfidf.transform(df['document'])\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Lexend', sans-serif; font-size: 24px;\">\n",
    "    <strong>4. Dimensionality Reduction</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualPCA:\n",
    "    def __init__(self, n_components=2):\n",
    "        self.n_components = n_components\n",
    "        self.components = None\n",
    "        self.mean = None\n",
    "        \n",
    "    def fit(self, X):\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        X_centered = X - self.mean\n",
    "        _, _, Vt = np.linalg.svd(X_centered, full_matrices=False)\n",
    "        self.components = Vt[:self.n_components]\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return (X - self.mean) @ self.components.T\n",
    "\n",
    "# Apply PCA\n",
    "pca = ManualPCA(n_components=2)\n",
    "pca.fit(tfidf_vectors)\n",
    "pca_vectors = pca.transform(tfidf_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Lexend', sans-serif; font-size: 24px;\">\n",
    "    <strong>5. Clustering Analysis</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ecfefe2d744fd3ade0f88b9eb809c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=6, description='n_clusters', max=10, min=2), IntSlider(value=42, descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from ipywidgets import interact\n",
    "\n",
    "class KMeansClustering:\n",
    "    def __init__(self, k=6, max_iters=100, random_state=42):\n",
    "        self.k = k\n",
    "        self.max_iters = max_iters\n",
    "        self.random_state = random_state\n",
    "        self.centroids = None\n",
    "        self.labels_ = None\n",
    "        \n",
    "    def _initialize_centroids(self, X):\n",
    "        np.random.seed(self.random_state)\n",
    "        centroids = [X[np.random.randint(X.shape[0])]]\n",
    "        for _ in range(1, self.k):\n",
    "            dist_sq = np.array([min(np.linalg.norm(x - c)**2 for c in centroids) for x in X])\n",
    "            probs = dist_sq / dist_sq.sum()\n",
    "            cumulative_probs = probs.cumsum()\n",
    "            r = np.random.rand()\n",
    "            centroids.append(X[np.where(cumulative_probs >= r)[0][0]])\n",
    "        return np.array(centroids)\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.centroids = self._initialize_centroids(X)\n",
    "        for _ in tqdm(range(self.max_iters), desc=\"Clustering Iterations\"):\n",
    "            distances = np.linalg.norm(X[:, np.newaxis] - self.centroids, axis=2)\n",
    "            labels = np.argmin(distances, axis=1)\n",
    "            new_centroids = np.array([\n",
    "                X[labels == i].mean(axis=0) if np.any(labels == i) \n",
    "                else self.centroids[i] \n",
    "                for i in range(self.k)\n",
    "            ])\n",
    "            if np.allclose(self.centroids, new_centroids):\n",
    "                break\n",
    "            self.centroids = new_centroids\n",
    "        self.labels_ = labels\n",
    "        return self\n",
    "\n",
    "# Interactive clustering control\n",
    "@interact(n_clusters=(2, 10, 1), random_seed=(0, 100, 1))\n",
    "def run_clustering(n_clusters=6, random_seed=42):\n",
    "    global df, pca_vectors\n",
    "    kmeans = KMeansClustering(k=n_clusters, random_state=random_seed).fit(pca_vectors)\n",
    "    df['cluster'] = kmeans.labels_\n",
    "\n",
    "    # Manual Silhouette Score Calculation\n",
    "    def manual_silhouette_score(X, labels):\n",
    "        n_samples = X.shape[0]\n",
    "        unique_labels = np.unique(labels)\n",
    "        n_clusters = len(unique_labels)\n",
    "\n",
    "        if n_clusters <= 1:\n",
    "            return 0.0\n",
    "\n",
    "        distances = np.sqrt((X[:, np.newaxis, :] - X) ** 2).sum(axis=2)\n",
    "        silhouette_scores = []\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            current_label = labels[i]\n",
    "            same_cluster = np.where(labels == current_label)[0]\n",
    "            same_cluster = same_cluster[same_cluster != i]\n",
    "            \n",
    "            a_i = np.mean(distances[i, same_cluster]) if len(same_cluster) > 0 else 0\n",
    "            b_i = np.min([np.mean(distances[i, np.where(labels == label)[0]]) \n",
    "                        for label in unique_labels if label != current_label])\n",
    "            \n",
    "            silhouette_scores.append((b_i - a_i) / max(a_i, b_i))\n",
    "\n",
    "        return np.nanmean(silhouette_scores)\n",
    "\n",
    "    # Manual NMI Calculation\n",
    "    def manual_nmi(true_labels, pred_labels):\n",
    "        contingency = pd.crosstab(pred_labels, true_labels)\n",
    "        pi_j = contingency.sum(axis=0) / len(true_labels)\n",
    "        pi_k = contingency.sum(axis=1) / len(true_labels)\n",
    "        \n",
    "        mi = 0\n",
    "        for k in contingency.index:\n",
    "            for j in contingency.columns:\n",
    "                p_jk = contingency.loc[k,j] / len(true_labels)\n",
    "                if p_jk > 0:\n",
    "                    mi += p_jk * np.log(p_jk / (pi_j[j] * pi_k[k]))\n",
    "        \n",
    "        h_true = -np.sum(pi_j * np.log(pi_j + 1e-12))\n",
    "        h_pred = -np.sum(pi_k * np.log(pi_k + 1e-12))\n",
    "        \n",
    "        return mi / np.sqrt(h_true * h_pred)\n",
    "\n",
    "    # Calculate both metrics\n",
    "    sil_score = manual_silhouette_score(pca_vectors, kmeans.labels_)\n",
    "    \n",
    "    label_encoder = {genre: idx for idx, genre in enumerate(df['genre'].unique())}\n",
    "    true_labels = df['genre'].map(label_encoder)\n",
    "    nmi_score = manual_nmi(true_labels, df['cluster'])\n",
    "\n",
    "    # Visualization with both metrics\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.scatterplot(\n",
    "        x=pca_vectors[:, 0], \n",
    "        y=pca_vectors[:, 1],\n",
    "        hue=df['genre'],\n",
    "        style=df['cluster'],\n",
    "        palette='viridis',\n",
    "        s=100,\n",
    "        edgecolor='w'\n",
    "    )\n",
    "    plt.title(f\"Clustering Result\\nSilhouette: {sil_score:.3f}, NMI: {nmi_score:.3f}\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Lexend', sans-serif; font-size: 24px;\">\n",
    "    <strong>6. Dynamic Metric Calculations</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "230da7f3b7bd4d8d88f6c5a8dd42ee34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=6, description='n_clusters', max=10, min=2), Output()), _dom_classes=('w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calculate_metrics(X, labels):\n",
    "    # Silhouette Score\n",
    "    def pairwise_dist(X):\n",
    "        return np.sqrt(((X[:, None] - X) ** 2).sum(axis=2))\n",
    "    \n",
    "    dist_matrix = pairwise_dist(X)\n",
    "    silhouette_scores = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        a = np.mean(dist_matrix[i, labels == labels[i]])\n",
    "        b = np.min([np.mean(dist_matrix[i, labels == k]) \n",
    "                   for k in np.unique(labels) if k != labels[i]])\n",
    "        silhouette_scores.append((b - a)/max(a, b))\n",
    "    \n",
    "    # Intra-cluster distance\n",
    "    intra_dists = [np.mean(pairwise_dist(X[labels == k])) \n",
    "                  for k in np.unique(labels)]\n",
    "    \n",
    "    return {\n",
    "        'silhouette': np.mean(silhouette_scores),\n",
    "        'intra_cluster': np.mean(intra_dists)\n",
    "    }\n",
    "\n",
    "# Add to clustering callback\n",
    "@interact(n_clusters=(2, 10, 1))\n",
    "def update_clusters(n_clusters=6):\n",
    "    kmeans = KMeansClustering(k=n_clusters).fit(pca_vectors)\n",
    "    metrics = calculate_metrics(pca_vectors, kmeans.labels_)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))  # Set figure size\n",
    "\n",
    "    # Scatter plot of clustered points\n",
    "    plt.scatter(pca_vectors[:, 0], pca_vectors[:, 1], c=kmeans.labels_, cmap='tab10', alpha=0.7)\n",
    "    \n",
    "    # Plot cluster centroids\n",
    "    plt.scatter(kmeans.centroids[:, 0], kmeans.centroids[:, 1], c='red', marker='x', s=200, label='Centroids')\n",
    "\n",
    "    # Update plot with metrics in title\n",
    "    plt.title(f\"Clusters: {n_clusters}\\nSilhouette: {metrics['silhouette']:.2f}\"\n",
    "              f\"\\nIntra-Cluster Distance: {metrics['intra_cluster']:.2f}\")\n",
    "\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "    plt.legend()\n",
    "    plt.show()  # Ensure plot renders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Lexend', sans-serif; font-size: 24px;\">\n",
    "    <strong>7. NMI Implementation</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmi_score(true_labels, pred_labels):\n",
    "    \"\"\"Compute Normalized Mutual Information (NMI) without sklearn\"\"\"\n",
    "    unique_true, true_indices = np.unique(true_labels, return_inverse=True)\n",
    "    unique_pred, pred_indices = np.unique(pred_labels, return_inverse=True)\n",
    "    \n",
    "    n = len(true_labels)\n",
    "    contingency = np.zeros((len(unique_pred), len(unique_true)), dtype=np.float64)\n",
    "    \n",
    "    for p, t in zip(pred_indices, true_indices):\n",
    "        contingency[p, t] += 1.0\n",
    "    \n",
    "    P = contingency / n\n",
    "    P_pred = np.sum(P, axis=1) + 1e-12\n",
    "    P_true = np.sum(P, axis=0) + 1e-12\n",
    "    \n",
    "    mi = 0.0\n",
    "    for i in range(len(unique_pred)):\n",
    "        for j in range(len(unique_true)):\n",
    "            if P[i, j] > 0.0:\n",
    "                mi += P[i, j] * np.log(P[i, j] / (P_pred[i] * P_true[j]))\n",
    "    \n",
    "    h_pred = -np.sum(P_pred * np.log(P_pred))\n",
    "    h_true = -np.sum(P_true * np.log(P_true))\n",
    "    return mi / np.sqrt(h_pred * h_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Lexend', sans-serif; font-size: 24px;\">\n",
    "    <strong>8. Genre Prediction System</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9def68193071404baf6edc39feea0e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clustering Iterations:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Neel Gupta\\AppData\\Local\\Temp\\ipykernel_18596\\524569465.py:45: DeprecationWarning: on_submit is deprecated. Instead, set the .continuous_update attribute to False and observe the value changing with: mywidget.observe(callback, 'value').\n",
      "  keyword_input.on_submit(on_submit)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5008594a4a34bb3b27060a6efd69e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Keywords:', layout=Layout(width='500px'), placeholder='Enter comma-separated keywo…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f34902dc18c4f35ba5c294b1ddf6442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import widgets\n",
    "class GenrePredictor:\n",
    "    def __init__(self, vectorizer, pca, kmeans, df):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.pca = pca\n",
    "        self.kmeans = kmeans\n",
    "        self.df = df\n",
    "        self.cluster_genres = df.groupby('cluster')['genre'].agg(pd.Series.mode)\n",
    "        \n",
    "    def predict(self, keywords):\n",
    "        valid_keywords = [word.lower().strip() for word in keywords \n",
    "                         if word in self.vectorizer.vocab]\n",
    "        if not valid_keywords:\n",
    "            return \"Unknown\"\n",
    "        \n",
    "        tfidf = self.vectorizer.transform([' '.join(valid_keywords)])[0]\n",
    "        if np.all(tfidf == 0):\n",
    "            return \"Unknown\"\n",
    "        \n",
    "        pca_proj = self.pca.transform(tfidf.reshape(1, -1))\n",
    "        distances = np.linalg.norm(self.kmeans.centroids - pca_proj, axis=1)\n",
    "        cluster = np.argmin(distances)\n",
    "        return self.cluster_genres.loc[cluster]\n",
    "\n",
    "# Initialize predictor\n",
    "predictor = GenrePredictor(tfidf, pca, KMeansClustering(k=6).fit(pca_vectors), df)\n",
    "\n",
    "# Created interactive widget\n",
    "keyword_input = widgets.Text(\n",
    "    placeholder='Enter comma-separated keywords',\n",
    "    description='Keywords:',\n",
    "    layout={'width': '500px'}\n",
    ")\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_submit(_):\n",
    "    output.clear_output()\n",
    "    with output:\n",
    "        keywords = [k.strip() for k in keyword_input.value.split(',')]\n",
    "        prediction = predictor.predict(keywords)\n",
    "        print(f\"Input Keywords: {keywords}\")\n",
    "        print(f\"Predicted Genre: {prediction}\")\n",
    "\n",
    "keyword_input.on_submit(on_submit)\n",
    "\n",
    "display(keyword_input, output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Lexend', sans-serif; font-size: 24px;\">\n",
    "    <strong>9. Advanced Visualizations</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b371c16816e34209bf3c9d34784e7eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=5, description='n_top', max=10, min=1), Output()), _dom_classes=('widget…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.cluster_analysis(n_top)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cluster_analysis(n_top):\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    \n",
    "    # --- Genre distribution per cluster (Heatmap for top n_top genres) ---\n",
    "    plt.subplot(1, 2, 1)\n",
    "    cluster_genre_counts = df.groupby(['cluster', 'genre']).size().unstack().fillna(0).astype(int)\n",
    "    total_genre_counts = cluster_genre_counts.sum()\n",
    "    top_genres = total_genre_counts.nlargest(n_top).index\n",
    "    cluster_genre_counts = cluster_genre_counts[top_genres]\n",
    "\n",
    "    sns.heatmap(cluster_genre_counts, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Genre Distribution Across Clusters (Top {n_top})')\n",
    "    plt.xlabel('Genre')\n",
    "    plt.ylabel('Cluster')\n",
    "\n",
    "    # --- Sum of frequencies of the top n_top words per cluster (Bar Chart) ---\n",
    "    plt.subplot(1, 2, 2)\n",
    "    from collections import Counter\n",
    "    top_words_freq_sum = []\n",
    "    \n",
    "    for cluster_id in sorted(df['cluster'].unique()):\n",
    "        cluster_docs = ' '.join(df[df['cluster'] == cluster_id]['document'])\n",
    "        words = [word for word in cluster_docs.split() if word in tfidf.vocab]\n",
    "        counter = Counter(words)\n",
    "        most_common_n = counter.most_common(n_top)\n",
    "        freq_sum = sum(count for _, count in most_common_n)\n",
    "        top_words_freq_sum.append(freq_sum)\n",
    "\n",
    "    df_top_words_freq_sum = pd.DataFrame({\n",
    "        'Cluster': sorted(df['cluster'].unique()),\n",
    "        f'Sum of Top {n_top} Word Frequencies': top_words_freq_sum\n",
    "    }).set_index('Cluster')\n",
    "\n",
    "    df_top_words_freq_sum.plot(kind='barh', legend=False, ax=plt.gca(), color='skyblue')\n",
    "    plt.xlabel(f'Sum of Top {n_top} Word Frequencies')\n",
    "    plt.ylabel('Cluster')\n",
    "    plt.title(f'Sum of Frequencies of Top {n_top} Words per Cluster')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Interactive widget to adjust `n_top`\n",
    "interact(cluster_analysis, n_top=(1, 10, 1))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
